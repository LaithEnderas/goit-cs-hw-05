# mapreduce_words.py
# downloads text by url, counts word frequency via mapreduce with threads, visualizes top words

import argparse
import re
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Iterable, List, Tuple

import matplotlib.pyplot as plt
import requests


WORD_RE = re.compile(r"[a-zA-Zа-яА-ЯіїєґІЇЄҐ']+")


def fetch_text(url: str, timeout: int = 10) -> str:
    r = requests.get(url, timeout=timeout)
    r.raise_for_status()
    r.encoding = r.apparent_encoding
    return r.text


def tokenize(text: str) -> List[str]:
    return [w.lower() for w in WORD_RE.findall(text)]


def chunked(items: List[str], chunk_size: int) -> Iterable[List[str]]:
    for i in range(0, len(items), chunk_size):
        yield items[i : i + chunk_size]


def map_fn(words: Iterable[str]) -> List[Tuple[str, int]]:
    return [(w, 1) for w in words]


def shuffle(pairs: Iterable[Tuple[str, int]]) -> Dict[str, int]:
    acc: Dict[str, int] = {}
    for w, c in pairs:
        acc[w] = acc.get(w, 0) + c
    return acc


def reduce_dicts(dicts: Iterable[Dict[str, int]]) -> Dict[str, int]:
    total: Dict[str, int] = {}
    for d in dicts:
        for w, c in d.items():
            total[w] = total.get(w, 0) + c
    return total


def map_reduce_word_count(text: str, workers: int = 8, chunk_size: int = 20_000) -> Dict[str, int]:
    words = tokenize(text)

    with ThreadPoolExecutor(max_workers=workers) as ex:
        mapped_chunks = ex.map(map_fn, chunked(words, chunk_size))

    partial_counts = (shuffle(chunk) for chunk in mapped_chunks)
    return reduce_dicts(partial_counts)


def visualize_top_words(freq: Dict[str, int], top_n: int) -> None:
    top = Counter(freq).most_common(top_n)
    if not top:
        print("no words found")
        return

    words = [w for w, _ in top][::-1]
    counts = [c for _, c in top][::-1]

    plt.figure(figsize=(10, 6))
    plt.barh(words, counts)
    plt.xlabel("frequency")
    plt.ylabel("words")
    plt.title(f"top {top_n} most frequent words")
    plt.tight_layout()
    plt.show()


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--url", required=True, help="url to download text from")
    parser.add_argument("--top", type=int, default=10, help="top n words to visualize")
    parser.add_argument("--workers", type=int, default=8, help="threads count")
    parser.add_argument("--chunk-size", type=int, default=20000, help="words per chunk for map step")
    args = parser.parse_args()

    try:
        text = fetch_text(args.url)
    except requests.RequestException as e:
        print(f"failed to download url: {e}")
        return

    freq = map_reduce_word_count(text, workers=args.workers, chunk_size=args.chunk_size)
    visualize_top_words(freq, args.top)


if __name__ == "__main__":
    main()
